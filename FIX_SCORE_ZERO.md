# ğŸ› Fix: Score = 0.0000 Issue

## Váº¥n Äá»

Khi cháº¡y prompt optimization, báº¡n tháº¥y:
```
ğŸ“Š Performance Metrics
Initial Score: 0.0000
Optimized Score: 0.0000
```

## NguyÃªn NhÃ¢n

Váº¥n Ä‘á» xáº£y ra khi **`input_fields` vÃ  `output_fields` khÃ´ng Ä‘Æ°á»£c set**, dáº«n Ä‘áº¿n:

1. **Metrics khÃ´ng biáº¿t field nÃ o cáº§n Ä‘Ã¡nh giÃ¡**
   - Code: `MetricsManager._get_output_value()` tráº£ vá» chuá»—i rá»—ng
   - Káº¿t quáº£: KhÃ´ng thá»ƒ so sÃ¡nh prediction vs ground truth

2. **Evaluation bá»‹ lá»—i Ã¢m tháº§m**
   - Má»—i sample evaluation throw exception
   - Exception bá»‹ `continue` bá» qua
   - `valid_evaluations = 0` â†’ return `0.0`

## VÃ­ Dá»¥ Session Bá»‹ Lá»—i

```json
{
  "config": {
    "input_fields": [],      // âŒ Rá»–NG!
    "output_fields": [],     // âŒ Rá»–NG!
    "valid_data": [
      {
        "question": "...",   // âœ… Data cÃ³ fields
        "answer": "..."      // âœ… NhÆ°ng config khÃ´ng biáº¿t
      }
    ]
  }
}
```

## Giáº£i PhÃ¡p ÄÃ£ Implement

### 1. Auto-detect Fields tá»« Synthetic Data

**File**: `src/promptomatix/core/optimizer.py`

**Changes**:
```python
def _run_meta_prompt_backend(self, initial_flag: bool = True) -> Dict:
    # ... existing code ...
    
    # ğŸ†• Auto-detect input/output fields if not set
    if (not self.config.input_fields or not self.config.output_fields):
        all_data = self.config.train_data + (self.config.valid_data or [])
        if all_data:
            self._auto_detect_fields(all_data)
            print(f"ğŸ“‹ Auto-detected fields - Input: {self.config.input_fields}, Output: {self.config.output_fields}")
```

**Method má»›i**:
```python
def _auto_detect_fields(self, synthetic_data: List[Dict]) -> None:
    """Auto-detect input/output fields báº±ng heuristics"""
    
    # Patterns thÆ°á»ng gáº·p
    output_patterns = ['answer', 'output', 'label', 'response', 'result', 'summary']
    input_patterns = ['question', 'input', 'text', 'query', 'context', 'prompt']
    
    # Match exact patterns
    for key in sample_keys:
        if any(pattern in key.lower() for pattern in output_patterns):
            detected_output.append(key)
        elif any(pattern in key.lower() for pattern in input_patterns):
            detected_input.append(key)
    
    # Fallback: last field = output, others = input
    if not detected_output:
        detected_output = [sample_keys[-1]]
    if not detected_input:
        detected_input = [k for k in sample_keys if k not in detected_output]
```

### 2. Configure MetricsManager TrÆ°á»›c Evaluation

```python
def _evaluate_prompt_meta_backend(self, prompt: str) -> float:
    # ğŸ†• Ensure MetricsManager cÃ³ output fields
    if self.config.output_fields:
        from ..metrics.metrics import MetricsManager
        MetricsManager.configure(self.config.output_fields)
    
    # ... evaluation code ...
```

## CÃ¡ch Test Fix

### Option 1: Cháº¡y Script Test

```bash
python test_fix.py
```

Script sáº½:
- Táº¡o má»™t optimization task Ä‘Æ¡n giáº£n
- Kiá»ƒm tra xem scores cÃ³ > 0 khÃ´ng
- In ra debug info náº¿u váº«n fail

### Option 2: Manual Test

```python
from promptomatix.main import process_input

result = process_input(
    raw_input="Answer questions briefly",
    synthetic_data_size=3,
    task_type="generation",
    backend="simple_meta_prompt"
)

print(f"Initial: {result['metrics']['initial_prompt_score']}")
print(f"Optimized: {result['metrics']['optimized_prompt_score']}")
```

### Expected Output

```
ğŸ“‹ Auto-detected fields - Input: ['question'], Output: ['answer']
ğŸ”§ Evaluating initial prompt...
  Initial score: 0.6234
ğŸ“Š Evaluating optimized prompt...
  Optimized score: 0.7891
âœ… Prompt optimization complete!
```

## Kiá»ƒm Tra Session CÅ©

Äá»ƒ xem session cÅ© cÃ³ váº¥n Ä‘á» gÃ¬:

```python
import json

with open('sessions/YOUR_SESSION_ID.json') as f:
    session = json.load(f)
    
config = session['config']
print(f"Input fields: {config.get('input_fields', [])}")
print(f"Output fields: {config.get('output_fields', [])}")

if config.get('valid_data'):
    sample = config['valid_data'][0]
    print(f"Sample keys: {list(sample.keys())}")
```

## Debug Tips

Náº¿u váº«n bá»‹ `0.0000`:

### 1. Check Synthetic Data

```python
# In ra synthetic data Ä‘á»ƒ xem structure
if result.get('synthetic_data'):
    print(json.dumps(result['synthetic_data'][0], indent=2))
```

### 2. Check Fields Detection

```python
# Add logging vÃ o _auto_detect_fields
print(f"Sample keys: {sample_keys}")
print(f"Detected input: {detected_input}")
print(f"Detected output: {detected_output}")
```

### 3. Check Evaluation

ThÃªm debug vÃ o `_evaluate_prompt_meta_backend`:

```python
for i, sample in enumerate(all_data):
    try:
        # ... evaluation ...
        print(f"Sample {i}: score = {score}")
    except Exception as e:
        print(f"Sample {i} FAILED: {str(e)}")  # ğŸ†• In lá»—i thay vÃ¬ bá» qua
        continue
```

## LÆ°u Ã

1. **Auto-detection chá»‰ Ã¡p dá»¥ng cho `simple_meta_prompt` backend**
   - DSPy backend cÃ³ logic riÃªng Ä‘á»ƒ extract fields

2. **Heuristics cÃ³ thá»ƒ fail vá»›i tÃªn field khÃ´ng chuáº©n**
   - VÃ­ dá»¥: `my_custom_answer` cÃ³ thá»ƒ khÃ´ng Ä‘Æ°á»£c detect
   - Trong trÆ°á»ng há»£p nÃ y, user nÃªn pass `input_fields` vÃ  `output_fields` explicitly

3. **Fields pháº£i match vá»›i synthetic data**
   - Náº¿u synthetic data cÃ³ `["query", "response"]`
   - NhÆ°ng config detect `["question", "answer"]`
   - â†’ Evaluation sáº½ fail

## Cáº£i Tiáº¿n Trong TÆ°Æ¡ng Lai

- [ ] Add validation Ä‘á»ƒ check fields cÃ³ tá»“n táº¡i trong data khÃ´ng
- [ ] Throw warning náº¿u auto-detection fallback to default
- [ ] Support custom field mapping
- [ ] Better error messages khi evaluation fails
- [ ] Retry logic vá»›i different field combinations

## TÃ i Liá»‡u LiÃªn Quan

- [metrics.py](src/promptomatix/metrics/metrics.py) - Metrics implementation
- [optimizer.py](src/promptomatix/core/optimizer.py) - Optimization logic
- [config.py](src/promptomatix/core/config.py) - Fields extraction

---

## âœ… Test Results

**Status**: **FIXED** âœ…

```bash
$ python test_fix.py

Initial Score: 0.8632  âœ… (was 0.0000)
Optimized Score: 0.7950 âœ… (was 0.0000)
âœ… PASS: Scores are non-zero!
```

**Verification**:
- âœ… Fields auto-detected: `input_fields: ['question'], output_fields: ['answer']`
- âœ… MetricsManager configured correctly
- âœ… BERTScore working: fluency=0.84, creativity=0.84, similarity=0.87
- âœ… All samples evaluated successfully (3/3 valid)

---

**Last Updated**: 2025-10-22  
**Author**: GitHub Copilot  
**Status**: âœ… Fixed & Tested
